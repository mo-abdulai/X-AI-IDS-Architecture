{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f503d5ea",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "298d1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle  # For saving and loading trained models\n",
    "from os import path\n",
    "\n",
    "# Importing libraries for data normalization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, OrdinalEncoder, LabelEncoder, MinMaxScaler, \n",
    "    OneHotEncoder, Normalizer, MaxAbsScaler, RobustScaler, PowerTransformer\n",
    ")\n",
    "\n",
    "# Importing libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing machine learning libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, precision_score, \n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing TensorFlow & Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Importing SHAP for explainability\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15141b52",
   "metadata": {},
   "source": [
    " # 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97cb3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 43)\n",
      "Dimensions of the Test set: (22544, 43)\n"
     ]
    }
   ],
   "source": [
    "# Define feature column names\n",
    "feature = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \n",
    "    \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \n",
    "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
    "    \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \n",
    "    \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\", \"difficulty\"\n",
    "]\n",
    "\n",
    "# Load training and test datasets\n",
    "train = \"../../Datasets/NSL-KDD/KDDTrain+.txt\"\n",
    "test = \"../../Datasets/NSL-KDD/KDDTest+.txt\"\n",
    "\n",
    "df = pd.read_csv(train, names=feature)\n",
    "df_test = pd.read_csv(test, names=feature)\n",
    "\n",
    "# Display dataset dimensions\n",
    "print('Dimensions of the Training set:', df.shape)\n",
    "print('Dimensions of the Test set:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02fea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a5b281c",
   "metadata": {},
   "source": [
    "# 3. Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d537490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['difficulty'], axis=1, inplace=True)\n",
    "df_test.drop(['difficulty'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce0ef4",
   "metadata": {},
   "source": [
    "# 4. Check Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fe33d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Label distribution in Training set:')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7396c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "rootkit              13\n",
      "xterm                13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "worm                  2\n",
      "loadmodule            2\n",
      "perl                  2\n",
      "sqlattack             2\n",
      "udpstorm              2\n",
      "phf                   2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nLabel distribution in Test set:')\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f007f99",
   "metadata": {},
   "source": [
    "# 5. Identify Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c4952f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Feature 'protocol_type' has 3 unique categories\n",
      "Feature 'service' has 70 unique categories\n",
      "Feature 'flag' has 11 unique categories\n",
      "Feature 'label' has 23 unique categories\n",
      "\n",
      "Distribution of service categories:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n",
      "\n",
      "Test set:\n",
      "Feature 'protocol_type' has 3 unique categories\n",
      "Feature 'service' has 64 unique categories\n",
      "Feature 'flag' has 11 unique categories\n",
      "Feature 'label' has 38 unique categories\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        print(f\"Feature '{col}' has {df[col].nunique()} unique categories\")\n",
    "\n",
    "print('\\nDistribution of service categories:')\n",
    "print(df['service'].value_counts().head())\n",
    "\n",
    "print('\\nTest set:')\n",
    "for col in df_test.columns:\n",
    "    if df_test[col].dtypes == 'object':\n",
    "        print(f\"Feature '{col}' has {df_test[col].nunique()} unique categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d31f8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values for each categorical column\n",
    "unique_protocol = sorted(df['protocol_type'].unique())\n",
    "unique_service = sorted(df['service'].unique())\n",
    "unique_flag = sorted(df['flag'].unique())\n",
    "\n",
    "# Prefix each category with its feature name\n",
    "protocol_columns = ['protocol_type_' + str(x) for x in unique_protocol]\n",
    "service_columns = ['service_' + str(x) for x in unique_service]\n",
    "flag_columns = ['flag_' + str(x) for x in unique_flag]\n",
    "\n",
    "# Combine all new column names\n",
    "one_hot_columns = protocol_columns + service_columns + flag_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805a4cd",
   "metadata": {},
   "source": [
    "# 6. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8d3d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   protocol_type_icmp  protocol_type_tcp  protocol_type_udp  service_IRC  \\\n",
      "0                 0.0                1.0                0.0          0.0   \n",
      "1                 0.0                0.0                1.0          0.0   \n",
      "2                 0.0                1.0                0.0          0.0   \n",
      "3                 0.0                1.0                0.0          0.0   \n",
      "4                 0.0                1.0                0.0          0.0   \n",
      "\n",
      "   service_X11  service_Z39_50  service_aol  service_auth  service_bgp  \\\n",
      "0          0.0             0.0          0.0           0.0          0.0   \n",
      "1          0.0             0.0          0.0           0.0          0.0   \n",
      "2          0.0             0.0          0.0           0.0          0.0   \n",
      "3          0.0             0.0          0.0           0.0          0.0   \n",
      "4          0.0             0.0          0.0           0.0          0.0   \n",
      "\n",
      "   service_courier  ...  flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  \\\n",
      "0              0.0  ...       0.0        0.0          0.0        0.0      0.0   \n",
      "1              0.0  ...       0.0        0.0          0.0        0.0      0.0   \n",
      "2              0.0  ...       0.0        0.0          0.0        0.0      1.0   \n",
      "3              0.0  ...       0.0        0.0          0.0        0.0      0.0   \n",
      "4              0.0  ...       0.0        0.0          0.0        0.0      0.0   \n",
      "\n",
      "   flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  \n",
      "0      0.0      0.0      0.0      1.0      0.0  \n",
      "1      0.0      0.0      0.0      1.0      0.0  \n",
      "2      0.0      0.0      0.0      0.0      0.0  \n",
      "3      0.0      0.0      0.0      1.0      0.0  \n",
      "4      0.0      0.0      0.0      1.0      0.0  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "\n",
    "df_categorical_values_enc = df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "testdf_categorical_values_enc = testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Encode categorical features using OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "# Fit and transform training categorical data\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(), columns=one_hot_columns)\n",
    "\n",
    "# Fit and transform test categorical data\n",
    "testdf_categorical_values_encenc = enc.transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(), columns=one_hot_columns)\n",
    "\n",
    "# Display first five rows of the updated dataframe\n",
    "print(df_cat_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc1df5",
   "metadata": {},
   "source": [
    "# Ensure consistent feature alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3936305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_services = set(df['service'])\n",
    "test_services = set(df_test['service'])\n",
    "missing_services = list(train_services - test_services)\n",
    "missing_service_cols = ['service_' + s for s in missing_services]\n",
    "\n",
    "for col in missing_service_cols:\n",
    "    testdf_cat_data[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353f0ce",
   "metadata": {},
   "source": [
    "# 7. Merge Encoded Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d856b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Set Shape: (125973, 123)\n",
      "Final Test Set Shape: (22544, 123)\n"
     ]
    }
   ],
   "source": [
    "# Merge encoded categorical data with the original dataset\n",
    "newdf = df.join(df_cat_data)\n",
    "newdf.drop(['protocol_type', 'service', 'flag'], axis=1, inplace=True)\n",
    "\n",
    "newdf_test = df_test.join(testdf_cat_data)\n",
    "newdf_test.drop(['protocol_type', 'service', 'flag'], axis=1, inplace=True)\n",
    "\n",
    "# Print the final shape of the dataset\n",
    "print(\"Final Training Set Shape:\", newdf.shape)\n",
    "print(\"Final Test Set Shape:\", newdf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "986aa4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
      "0         0        491          0     0               0       0    0   \n",
      "1         0        146          0     0               0       0    0   \n",
      "2         0          0          0     0               0       0    0   \n",
      "3         0        232       8153     0               0       0    0   \n",
      "4         0        199        420     0               0       0    0   \n",
      "\n",
      "   num_failed_logins  logged_in  num_compromised  ...  flag_REJ  flag_RSTO  \\\n",
      "0                  0          0                0  ...       0.0        0.0   \n",
      "1                  0          0                0  ...       0.0        0.0   \n",
      "2                  0          0                0  ...       0.0        0.0   \n",
      "3                  0          1                0  ...       0.0        0.0   \n",
      "4                  0          1                0  ...       0.0        0.0   \n",
      "\n",
      "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
      "0          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "1          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "2          0.0        0.0      1.0      0.0      0.0      0.0      0.0   \n",
      "3          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "4          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "\n",
      "   flag_SH  \n",
      "0      0.0  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "\n",
      "[5 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows of the processed dataset\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe302e",
   "metadata": {},
   "source": [
    "# 8. Encode Labels into Attack Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3e5a042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "attack_mapping = {\n",
    "    'normal': 0, \n",
    "    'neptune': 1, 'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1, 'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "    'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2, 'mscan': 2, 'saint': 2,\n",
    "    'ftp_write': 3, 'guess_passwd': 3, 'imap': 3, 'multihop': 3, 'phf': 3, 'spy': 3, 'warezclient': 3, 'warezmaster': 3, \n",
    "    'sendmail': 3, 'named': 3, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3, 'httptunnel': 3,\n",
    "    'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4, 'ps': 4, 'sqlattack': 4, 'xterm': 4\n",
    "}\n",
    "\n",
    "newdf['label'] = newdf['label'].replace(attack_mapping)\n",
    "newdf_test['label'] = newdf_test['label'].replace(attack_mapping)\n",
    "\n",
    "print(newdf['label'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9da692",
   "metadata": {},
   "source": [
    "# 9. Normalize Data using Standard Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062798bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aa1b52f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
      "0 -0.110249  -0.007679  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "1 -0.110249  -0.007737  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "2 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "3 -0.110249  -0.007723  -0.002891 -0.014089       -0.089486 -0.007736   \n",
      "4 -0.110249  -0.007728  -0.004814 -0.014089       -0.089486 -0.007736   \n",
      "\n",
      "        hot  num_failed_logins  logged_in  num_compromised  ...  flag_REJ  \\\n",
      "0 -0.095076          -0.027023  -0.809262        -0.011664  ... -0.312889   \n",
      "1 -0.095076          -0.027023  -0.809262        -0.011664  ... -0.312889   \n",
      "2 -0.095076          -0.027023  -0.809262        -0.011664  ... -0.312889   \n",
      "3 -0.095076          -0.027023   1.235694        -0.011664  ... -0.312889   \n",
      "4 -0.095076          -0.027023   1.235694        -0.011664  ... -0.312889   \n",
      "\n",
      "   flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1   flag_S2   flag_S3  \\\n",
      "0   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906 -0.031767 -0.019726   \n",
      "1   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906 -0.031767 -0.019726   \n",
      "2   -0.11205    -0.028606  -0.139982  1.616978 -0.053906 -0.031767 -0.019726   \n",
      "3   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906 -0.031767 -0.019726   \n",
      "4   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906 -0.031767 -0.019726   \n",
      "\n",
      "    flag_SF   flag_SH  \n",
      "0  0.825150 -0.046432  \n",
      "1  0.825150 -0.046432  \n",
      "2 -1.211901 -0.046432  \n",
      "3  0.825150 -0.046432  \n",
      "4  0.825150 -0.046432  \n",
      "\n",
      "[5 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify numeric columns (excluding the label column)\n",
    "numeric_columns = newdf.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns.remove('label')  # Exclude label column from scaling\n",
    "\n",
    "# Standardize the training and test sets\n",
    "newdf[numeric_columns] = scaler.fit_transform(newdf[numeric_columns])\n",
    "newdf_test[numeric_columns] = scaler.transform(newdf_test[numeric_columns])\n",
    "\n",
    "# Display first few rows to verify standardization\n",
    "print(newdf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e483b",
   "metadata": {},
   "source": [
    "# 10. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "224ccb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (125973, 122) y_train shape: (125973, 1)\n",
      "X_test shape: (22544, 122) y_test shape: (22544, 1)\n"
     ]
    }
   ],
   "source": [
    "multi_data = newdf.copy()\n",
    "multi_data_test = newdf_test.copy()\n",
    "\n",
    "y_train_multi = multi_data[['label']]\n",
    "X_train_multi = multi_data.drop(columns=['label'])\n",
    "\n",
    "y_test_multi = multi_data_test[['label']]\n",
    "X_test_multi = multi_data_test.drop(columns=['label'])\n",
    "\n",
    "print('X_train shape:', X_train_multi.shape, 'y_train shape:', y_train_multi.shape)\n",
    "print('X_test shape:', X_test_multi.shape, 'y_test shape:', y_test_multi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f748a2",
   "metadata": {},
   "source": [
    "# 11. Convert Labels to One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3075432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (125973, 122), y_train Shape: (125973,)\n",
      "X_test Shape: (22544, 122), y_test Shape: (22544,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "X_train = newdf.drop(columns=['label'])\n",
    "y_train = newdf['label']\n",
    "\n",
    "X_test = newdf_test.drop(columns=['label'])\n",
    "y_test = newdf_test['label']\n",
    "\n",
    "print(f'X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}')\n",
    "print(f'X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
